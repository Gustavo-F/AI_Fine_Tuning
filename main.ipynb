{
 "cells": [
  {
   "cell_type": "code",
   "id": "7926fb0e-5e3f-43a8-94e7-81cb47762a21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T17:45:56.195005Z",
     "start_time": "2025-12-29T17:45:54.333426Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "data = pd.read_csv(\"datasets/dataset_v1.csv\")\n",
    "tuning_examples = []\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    tuning_examples.append(f\"<|user|>\\n{row['instruction']}:\\n{row['input']}\\n<|assistant|>\\n{row['output']}\\n<|endoftext|>\")\n",
    "\n",
    "dataset = Dataset.from_dict({\"text\": tuning_examples})\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gustavo/Git/AI_Fine_Tuning/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "20641993-eab0-4051-8396-92add3384757",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T17:46:13.706635Z",
     "start_time": "2025-12-29T17:45:56.200180Z"
    }
   },
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    max_seq_length = 2048,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.12.8: Fast Llama patching. Transformers: 4.57.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4060. Num GPUs = 1. Max memory: 7.617 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.1+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.5.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "a6ba4c9e11857614",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T17:46:16.769630Z",
     "start_time": "2025-12-29T17:46:13.711871Z"
    }
   },
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 64,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha = 64 * 2,\n",
    "    lora_dropout = 0,\n",
    "    bias = 'none',\n",
    "    use_gradient_checkpointing = 'unsloth'\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.12.8 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T17:46:20.908095Z",
     "start_time": "2025-12-29T17:46:16.774169Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    train_dataset = dataset,\n",
    "    tokenizer = tokenizer,\n",
    "    dataset_text = 'text',\n",
    "    max_seq_length = 2048,\n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 10,\n",
    "        max_steps = 60,\n",
    "        num_train_epochs = 3,\n",
    "        logging_steps = 1,\n",
    "        output_dir = 'outputs',\n",
    "        optim = 'adamw_8bit'\n",
    "    ),\n",
    ")"
   ],
   "id": "c668ea5195a5028e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Tokenizing [\"text\"] (num_proc=16): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:03<00:00,  4.56 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Padding-free auto-enabled, enabling faster training.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T17:47:38.105059Z",
     "start_time": "2025-12-29T17:46:20.917438Z"
    }
   },
   "cell_type": "code",
   "source": "trainer.train()",
   "id": "b204241fc5fcebfe",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 17 | Num Epochs = 20 | Total steps = 60\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 97,255,424 of 3,310,005,248 (2.94% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 01:13, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.037200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.454700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.662700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.466700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.650300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.076700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.534600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.986200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.709900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.645000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.941200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.437400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.286600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.367500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.264700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.992200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.049800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.759700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.809100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.699800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.701600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.647700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.542700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.550400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.461200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.512300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.281900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.382200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.333800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.306500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.303100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.199500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.240900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.217500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.177900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.102400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.162500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.094600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.119700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.098000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.092400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.081100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.069600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.067900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.148800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.054700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.065700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.047500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.044700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.050600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.088800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.057000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.033600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.062600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.058000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.034200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.021400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.034500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.035900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.115100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": "414e930f73137fcdf9a82bdf289a59f3"
     }
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=60, training_loss=0.7255418534701069, metrics={'train_runtime': 76.2842, 'train_samples_per_second': 6.292, 'train_steps_per_second': 0.787, 'total_flos': 824412316139520.0, 'train_loss': 0.7255418534701069, 'epoch': 20.0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T17:47:38.850637Z",
     "start_time": "2025-12-29T17:47:38.119556Z"
    }
   },
   "cell_type": "code",
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Converta esse script do IBM RPA para Python: \\n\\ndefVar --name fruit --type String --value \\\"Melancia Comprida\\\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(input_ids=inputs, max_new_tokens=512, use_cache=True, temperature=0.7, do_sample=True, top_p=0.9)\n",
    "response = tokenizer.batch_decode(outputs)[0]\n",
    "\n",
    "print(response)"
   ],
   "id": "65ff83c36597036a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 29 Dec 2025\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Converta esse script do IBM RPA para Python: \n",
      "\n",
      "defVar --name fruit --type String --value \"Melancia Comprida\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "fruit = \"Melancia Comprida\"\n",
      "print(fruit)<|eot_id|>\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T17:50:41.325346Z",
     "start_time": "2025-12-29T17:47:38.858087Z"
    }
   },
   "cell_type": "code",
   "source": "model.save_pretrained_gguf(\"finetuned_model\", tokenizer, quantization_method=\"q4_k_m\", maximum_memory_usage=0.3)",
   "id": "3deb214934decb73",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging model weights to 16-bit format...\n",
      "Found HuggingFace hub cache directory: /home/gustavo/.cache/huggingface/hub\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 1 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking cache directory for required files...\n",
      "Cache check failed: model-00001-of-00002.safetensors not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n",
      "Checking cache directory for required files...\n",
      "Cache check failed: tokenizer.model not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Preparing safetensor model files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [01:50<00:00, 55.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: tokenizer.model not found (this is OK for non-SentencePiece models)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:12<00:00,  6.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merge process complete. Saved to `/home/gustavo/Git/AI_Fine_Tuning/finetuned_model`\n",
      "Unsloth: Converting to GGUF format...\n",
      "==((====))==  Unsloth: Conversion from HF to GGUF information\n",
      "   \\\\   /|    [0] Installing llama.cpp might take 3 minutes.\n",
      "O^O/ \\_/ \\    [1] Converting HF to GGUF bf16 might take 3 minutes.\n",
      "\\        /    [2] Converting GGUF bf16 to ['q4_k_m'] might take 10 minutes each.\n",
      " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
      "\n",
      "Unsloth: llama.cpp found in the system. Skipping installation.\n",
      "Unsloth: Preparing converter script...\n",
      "Unsloth: [1] Converting model into bf16 GGUF format.\n",
      "This might take 3 minutes...\n",
      "Unsloth: Initial conversion completed! Files: ['llama-3.2-3b-instruct.BF16.gguf']\n",
      "Unsloth: [2] Converting GGUF bf16 into q4_k_m. This might take 10 minutes...\n",
      "Unsloth: Model files cleanup...\n",
      "Unsloth: All GGUF conversions completed successfully!\n",
      "Generated files: ['llama-3.2-3b-instruct.Q4_K_M.gguf']\n",
      "Unsloth: example usage for text only LLMs: llama-cli --model llama-3.2-3b-instruct.Q4_K_M.gguf -p \"why is the sky blue?\"\n",
      "Unsloth: Saved Ollama Modelfile to current directory\n",
      "Unsloth: convert model to ollama format by running - ollama create model_name -f ./Modelfile - inside current directory.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'save_directory': 'finetuned_model',\n",
       " 'gguf_files': ['llama-3.2-3b-instruct.Q4_K_M.gguf'],\n",
       " 'modelfile_location': '/home/gustavo/Git/AI_Fine_Tuning/Modelfile',\n",
       " 'want_full_precision': False,\n",
       " 'is_vlm': False,\n",
       " 'fix_bos_token': False}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
